# -*- coding: utf-8 -*-
"""FixBenerInsyaAllah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aEXAl8ez_CKwQzd_6sN_eFT-szWoaY8G
"""

#Haris Amaldi
#Dilarang mengcopy code. PLAGIARISME ITU HARAM!!!!

#Membuka file dataset
!unzip datafilm.zip

#Mengimport fungsi-fungsi yang dibutuhkan dan mencari banyaknya data film dan pengguna yang tercatat.
import os
import plotly.express as px
import numpy as np
from datetime import datetime
import pandas as pd
import plotly.graph_objects as go
import seaborn as sns
import re
from sklearn.neighbors import NearestNeighbors
import random
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

film = pd.read_csv("movies.csv")
rating = pd.read_csv("ratings.csv")

print('Jumlah data film: ', len(film.movieId.unique()))
print('Jumlah data pengguna: ', len(rating.userId.unique()))

#Melihat bentuk data film
film.info()

#Melihat bentuk data rating
rating.info()

# Menggabungkan seluruh movieId pada kategori film dan rating
film_all = np.concatenate((
    film.movieId.unique(),
    rating.movieId.unique()
))
 
# Mengurutkan data dan menghapus data yang sama
film_all = np.sort(np.unique(film_all))
 
print('Jumlah seluruh data film berdasarkan movieID: ', len(film_all))

filmrating = pd.merge(film, rating , on='movieId', how='left')
filmrating

print("Rating yang tersedia:", filmrating.rating.unique())

filmrating.isnull().sum()

filmrating = filmrating.dropna()
filmrating

# Memisahkan tahun rilis film menjadi 1 kolom sendiri
filmrating['tahunrilis'] = filmrating.title.str.extract('.*\((.*)\).*')
filmrating.head()

# Menghapus tahun dari kolom judul
filmrating['title'] = filmrating.title.str.split('(').str[0].str[:-1]
filmrating.head()

#mengubah timestamp menjadi data yang bisa dibaca
def UNIX_to_Readable(df):
    return pd.to_datetime(datetime.fromtimestamp(df).strftime('%Y-%m-%d %H:%M:%S'))


filmrating.timestamp = filmrating.timestamp.apply(UNIX_to_Readable)
filmrating.head()

# Menghapus nilai rating berkoma (Tidak bulat)
filmrating.rating = np.ceil(filmrating.rating)
print("Rating yang tersedia:", filmrating.rating.unique())

#Pengujian penambahan Jumlah rating agar ketahuan apakah ada yang hanya dirating kurang dari 10 pengguna.
filmrating.groupby('movieId').sum()

#Ada yang jumlah ratingnya dibawah rentang 10-50. Harus dihapus terlebih dahulu biar lebih cepat dan efektif.
movieFrequency_greater_10 = filmrating['movieId'].value_counts()[filmrating['movieId'].value_counts() >= 10].index
filmrating = filmrating[filmrating.movieId.isin(movieFrequency_greater_10)]

#Preparasi data
preparasi = filmrating
preparasi.sort_values('movieId')

preparasi = preparasi.drop_duplicates('movieId')
preparasi

Idfilm = preparasi['movieId'].tolist()
Judul = preparasi['title'].tolist()
Genre = preparasi['genres'].tolist()

print(len(Idfilm))
print(len(Judul))
print(len(Genre))

#Membuat dictionary baru
dictbaru = pd.DataFrame({
    'idfilm': Idfilm,
    'Judulfilm': Judul,
    'Genrefilm': Genre
})
dictbaru

"""Ini content based filtering"""

data = dictbaru
data.sample(5)

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data genre film
tf.fit(data['Genrefilm']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['Genrefilm']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan genre film
# Baris diisi dengan judul film
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.Judulfilm
).sample(21, axis=1).sample(10, axis=0)

from sklearn.metrics.pairwise import cosine_similarity
 
# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul film
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['Judulfilm'], columns=data['Judulfilm'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap judul film
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def film_recommendations(nama_film, similarity_data=cosine_sim_df, items=data[['Judulfilm', 'Genrefilm']], k=5):
 
    index = similarity_data.loc[:,nama_film].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_film agar nama film yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_film, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

data[data.Judulfilm.eq('Toy Story')]

film_recommendations('Toy Story')

# Import library
import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

# Membaca dataset
 
df = preparasi
df

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah movieID menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()
 
# Melakukan proses encoding placeID
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
 
# Melakukan proses encoding angka ke placeID
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

# Mapping userID ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)
 
# Mapping placeID ke dataframe movie
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah rmovie
num_movie = len(movie_encoded_to_movie)
print(num_movie)
 
# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['rating'])
 
# Nilai maksimal rating
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df[['user', 'movie']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_movie + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
from keras import backend as K

 
history = model.fit(
    x = K.cast_to_floatx(x_train),
    y = K.cast_to_floatx(y_train),
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

movie_df = dictbaru
df = filmrating

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_visited_by_user = df[df.userId == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
movie_not_visited = movie_df[~movie_df['idfilm'].isin(movie_visited_by_user.movieId.values)]['idfilm'] 
movie_not_visited = list(
    set(movie_not_visited)
    .intersection(set(movie_to_movie_encoded.keys()))
)
 
movie_not_visited = [[movie_to_movie_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)

user_movie_array

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)
 
top_movie_user = (
    movie_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = movie_df[movie_df['idfilm'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.Judulfilm, ':', row.Genrefilm)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_movie = movie_df[movie_df['idfilm'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.Judulfilm, ':', row.Genrefilm)